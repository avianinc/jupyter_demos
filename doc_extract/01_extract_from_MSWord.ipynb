{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will go into detail on extracting information from MSWord Documents locally. Since many companies and roles are inseparable from the Microsoft Office Suite, this is a useful reference for anyone faced with data transferred through .doc or .docx formats.\n",
    "\n",
    "This is a sister-blog to my entry about Thomas Edison State University's (TESU) open source materials accessibility initiative. <a href = \"https://medium.com/@NatalieOlivo/preserving-web-content-of-links-provided-in-a-word-doc-using-aws-services-ec2-and-s3-2c4f0cee0a26\">Medium post</a> / <a href = \"https://github.com/nmolivo/tesu_scraper\">Github repository</a><br><br>\n",
    "The initial blog I composed for this project was specific and borders on the line of unwieldy, so this blog is the first in a series I will be writing to take deep dives into separate aspects of the TESU project and make the material more accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#specific to extracting information from word documents\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "#useful tool for extracting information from XML\n",
    "import re\n",
    "\n",
    "#to pretty print our xml:\n",
    "import xml.dom.minidom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample word document can be found in this github repository. Let's find it using the packages we've imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#to check files in the current directory, use a single period\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#to check files in the directory above the current directory, use double periods\n",
    "os.listdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#the sample word document is in the folder entitled \"docs\"\n",
    "#os.listdir('../docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we can find where the word document is!<Br><Br>\n",
    "We will now use the <a href = \"https://docs.python.org/3/library/zipfile.html\">zipfile</a> library to help us read our document. The defaults are listed below, and they're all good for our purposes of reading the word document.<br><br>\n",
    "```python\n",
    "class zipfile.ZipFile(file, mode='r', compression=ZIP_STORED, allowZip64=True, compresslevel=None)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document = zipfile.ZipFile('sow_example.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now to turn this into xml using \n",
    "```python \n",
    "ZipFile.read(name, pwd=None)```\n",
    "\n",
    "As you can see, we need a name. This is different than the filename. This is how to direct ZipFile to read your file in the correct format. For our purposes, we'll want to see the xml that makes up the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "document.namelist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see what kind of information is stored in our document, let's test a few.\n",
    "I'm going to pretty print it, so it's a little more legible. This work is courtesy of answer from user <a href = \"https://stackoverflow.com/users/47775/nick-bolton\">Nick Bolton</a> on StackOverflow Question: <a href = \"https://stackoverflow.com/questions/749796/pretty-printing-xml-in-python\">Pretty Printing XML in Python</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#name = 'word/people.xml'\n",
    "#we can see who the document author is: Stephen C. Phillips\n",
    "uglyXml = xml.dom.minidom.parseString(document.read('word/document.xml')).toprettyxml(indent='  ')\n",
    "\n",
    "text_re = re.compile('>\\n\\s+([^<>\\s].*?)\\n\\s+</', re.DOTALL)    \n",
    "prettyXml = text_re.sub('>\\g<1></', uglyXml)\n",
    "\n",
    "print(prettyXml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#name = 'word/fontTable.xml'\n",
    "#This looks like the fonts used in the document style\n",
    "uglyXml = xml.dom.minidom.parseString(document.read('word/document.xml')).toprettyxml(indent='  ')\n",
    "\n",
    "text_re = re.compile('>\\n\\s+([^<>\\s].*?)\\n\\s+</', re.DOTALL)    \n",
    "prettyXml = text_re.sub('>\\g<1></', uglyXml)\n",
    "\n",
    "print(prettyXml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#ok cool let's get the xml that has the text contained in the document\n",
    "#name = 'word/document.xml'\n",
    "uglyXml = xml.dom.minidom.parseString(document.read('word/document.xml')).toprettyxml(indent='  ')\n",
    "\n",
    "text_re = re.compile('>\\n\\s+([^<>\\s].*?)\\n\\s+</', re.DOTALL)    \n",
    "prettyXml = text_re.sub('>\\g<1></', uglyXml)\n",
    "\n",
    "print(prettyXml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the scope of this contract, we'll need to find all hyperlinks stored in this document and add them to a list. Let's take a look at what hyperlinks look like in the xml:\n",
    "```\n",
    "      <w:hyperlink r:id=\"rId27\">\n",
    "        <w:r w:rsidRPr=\"0005387D\">\n",
    "          <w:rPr>\n",
    "            <w:rFonts w:ascii=\"Arial\" w:cs=\"Arial\" w:eastAsia=\"Arial\" w:hAnsi=\"Arial\"/>\n",
    "            <w:color w:val=\"0000FF\"/>\n",
    "            <w:sz w:val=\"20\"/>\n",
    "            <w:szCs w:val=\"20\"/>\n",
    "            <w:u w:val=\"single\"/>\n",
    "          </w:rPr>\n",
    "          <w:t>https://cyber.harvard.edu/getinvolved/jobs/communicationsmanager</w:t>\n",
    "        </w:r>\n",
    "      </w:hyperlink>\n",
    "```\n",
    "\n",
    "A simple pattern I notice is that they all start with characters `>http` and end with characters `</`<br> Now we can convert our xml to a string and use regex to collect all text between those characters.\n",
    "\n",
    "To help with the regex I'll need to accomplish our goal of collecting all text between the aforementioned characters, I used the following StackOverflow question, which contains what I am looking for in the initial ask: <a href = \"https://stackoverflow.com/questions/1454913/regular-expression-to-find-a-string-included-between-two-characters-while-exclud\">Regular Expression to find a string included between two characters while EXCLUDING the delimiters.</a> \n",
    "\n",
    "While I do want to keep the `http`, I do not want to keep the `<` or `>`. I will make these modifications to my list items using list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first to turn the xml content into a string:\n",
    "xml_content = document.read('word/document.xml')\n",
    "document.close()\n",
    "xml_str = str(xml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# link_list = re.findall('http.*?\\<',xml_str)[1:]\n",
    "# link_list = [x[:-1] for x in link_list]\n",
    "shall_list = re.findall('shall', xml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "shall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(shall_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now collected all the URLS from this word document! Check out <a href= \"https://github.com/nmolivo/tesu_scraper/blob/master/01_scraper.ipynb\">the Notebook</a> updloaded to <a href = \"https://github.com/nmolivo/tesu_scraper\">the TESU Scraper Repo</a>, where we use the technique covered in this repository to collect a link list for each word document contained in a folder stored on an AWS S3 bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Natalie Olivo\n",
    "<a href = \"https://www.linkedin.com/in/natalie-olivo-82548951/\">LinkedIn</a> | <a href = \"https://github.com/nmolivo\">GitHub</a> | <a href = \"https://medium.com/@NatalieOlivo\">Blog</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for: ['doc2txt']\n",
      "\n",
      "\n",
      "Pinned packages:\n",
      "  - python 3.9.*\n",
      "\n",
      "\n",
      "Encountered problems while solving:\n",
      "  - nothing provides requested doc2txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!mamba install --yes --prefix {sys.prefix} -c conda-forge doc2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx2txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_135564\\3007634513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxml_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdocu_Regex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocu_Regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docx2txt'"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "test_doc = docx2txt.process(xml_content)\n",
    "docu_Regex = re.compile(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d')\n",
    "mo = docu_Regex.findall(test_doc)\n",
    "print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
